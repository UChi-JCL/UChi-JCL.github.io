<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Junchen&#39;s Lab</title>
    <link>https://uchi-jcl.github.io/group-website/</link>
      <atom:link href="https://uchi-jcl.github.io/group-website/index.xml" rel="self" type="application/rss+xml" />
    <description>Junchen&#39;s Lab</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://uchi-jcl.github.io/group-website/media/icon_hua25ef39b30b8f32ff2a6fd2e1eed54f5_1032324_512x512_fill_lanczos_center_3.png</url>
      <title>Junchen&#39;s Lab</title>
      <link>https://uchi-jcl.github.io/group-website/</link>
    </image>
    
    <item>
      <title>Example Event</title>
      <link>https://uchi-jcl.github.io/group-website/event/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/event/example/</guid>
      <description>&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://docs.hugoblox.com/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://docs.hugoblox.com/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including page elements such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving</title>
      <link>https://uchi-jcl.github.io/group-website/publication/cachegen/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/cachegen/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Hanchen presented CacheGen paper at SIGCOMM&#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/hanchen_sigcomm/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/hanchen_sigcomm/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; CacheGen is a LLM KV cache compression and streaming system that saves storage size and reduces transfer time. By utilizing delta compression and information theory, CacheGen is able to further compress KV cache by up to 4.3x on top of previous machine learning techniques with option to incur NO additional loss. &lt;/p&gt;
&lt;p&gt;Link to a pre-recorded version:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/H4_OUWvdiNo?si=oWtNqMszSVOqKeAV&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>Junchen&#39;s talk on Knowledge-Delivery Networks at ByteDance and APNet&#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/junchen_bytedance/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/junchen_bytedance/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; Imagine once an LLM has processed a document, the &#34;knowledge&#34; can be instantly shared with other LLM instances. Unfortunately, today, LLMs must read the same long document multiple times, causing a significant slowdown. We introduce a new Knowledge Delivery Network that enables LLMs to efficiently share their digested knowledge, in the form of KV caches, so only one LLM instance needs to process (prefill) each document. The key challenge is how to store the KV caches cheaply and serve them quickly. Instead of keeping the KV caches of all reusable chunks in GPU or CPU memory, we show that with careful design and implementation, storing them on cheaper devices is not only economically superior but also delivers significant reductions in LLM serving delay, especially the time to the first token. &lt;/p&gt;
&lt;br&gt; 
Slides link : &lt;a href=&#34;https://people.cs.uchicago.edu/~junchenj/docs/LLM_Jiang_v10_APNet.pptx&#34;&gt; Download &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Eloquent: A More Robust Transmission Scheme for LLM Token Streaming</title>
      <link>https://uchi-jcl.github.io/group-website/publication/chatterbox/</link>
      <pubDate>Sat, 03 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/chatterbox/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Yuhan Liu presented ChameleonAPI at OSDI&#39;24.</title>
      <link>https://uchi-jcl.github.io/group-website/post/yuhan_osdi/</link>
      <pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/yuhan_osdi/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; ChameleonAPI provides application developers with a parser that automatically analyzes the application to produce an abstract of its decision process, which is then used to devise an application-specific loss function that only penalizes API output errors critical to the application. ChameleonAPI uses the loss function to efficiently train a neural network model customized for each application and deploys it to serve API invocations from the respective application via existing interface. Compared to a baseline that selects the best-of-all commercial ML API, ChameleonAPI reduces incorrect application decisions by 43%. &lt;/p&gt;
&lt;br&gt; 
Check out the paper for more details : &lt;a href=&#34;https://www.usenix.org/conference/osdi24/presentation/liu&#34;&gt; ChameleonAPI &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Zhengxu Xia successfully completes his PhD candidacy exam at UChicago.</title>
      <link>https://uchi-jcl.github.io/group-website/post/zhengxu_cand/</link>
      <pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/zhengxu_cand/</guid>
      <description>&lt;p&gt;Congratulations Zhengxu!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Yihua Cheng starts his internship at Conviva.</title>
      <link>https://uchi-jcl.github.io/group-website/post/yihua_conv/</link>
      <pubDate>Tue, 04 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/yihua_conv/</guid>
      <description>&lt;p&gt;Congratulations Yihua!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion</title>
      <link>https://uchi-jcl.github.io/group-website/publication/cacheblend/</link>
      <pubDate>Mon, 03 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/cacheblend/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion out on ArXiv.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/cacheblend/</link>
      <pubDate>Mon, 03 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/cacheblend/</guid>
      <description>&lt;p&gt;Check out the paper for more details : &lt;a href=&#34;https://arxiv.org/abs/2405.16444&#34;&gt; Paper &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Yuhan Liu starts her internship at Microsoft Research.</title>
      <link>https://uchi-jcl.github.io/group-website/post/yuhan_msr/</link>
      <pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/yuhan_msr/</guid>
      <description>&lt;p&gt;Congratulations Yuhan!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Yuhan Liu successfully completes her MS exam at UChicago.</title>
      <link>https://uchi-jcl.github.io/group-website/post/yuhan_master/</link>
      <pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/yuhan_master/</guid>
      <description>&lt;p&gt;Congratulations Yuhan!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Junchen&#39;s talk on the LLM Caching Layer at Databricks and Alluxio’s AI/ML Infra meetup.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/junchen_databricks/</link>
      <pubDate>Wed, 22 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/junchen_databricks/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; Prefill in LLM inference is known to be resource-intensive, especially for long LLM inputs. While better scheduling can mitigate prefill’s impact, it would be fundamentally better to avoid (most of) prefill. This talk introduces our preliminary effort towards drastically minimizing prefill delay for LLM inputs that naturally reuse text chunks, such as in retrieval-augmented generation. While keeping the KV cache of all text chunks in memory is difficult, we show that it is possible to store them on cheaper yet slower storage. By improving the loading process of the reused KV caches, we can still significantly speed up prefill delay while maintaining the same generation quality. &lt;/.p&gt;
&lt;p&gt;Talk link :&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/a5YH2RCe4NU?si=7hRisB8JXXl19grx&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Slides link :&lt;/p&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/eOSyDpGElSFWvB&#34; width=&#34;560&#34; height=&#34;315&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/slideshow/ai-ml-infra-meetup-reducing-prefill-for-llm-serving-in-rag/269308079&#34; title=&#34;&#34; target=&#34;_blank&#34;&gt;AI/ML Infra Meetup | Reducing Prefill for LLM Serving in RAG&lt;/a&gt; &lt;/strong&gt; &lt;/strong&gt; &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Using Cached Knowledge for Large Language Model Serving with CacheFuse
</title>
      <link>https://uchi-jcl.github.io/group-website/projects/cachefuse/</link>
      <pubDate>Tue, 21 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/projects/cachefuse/</guid>
      <description>&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt; Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, and when they are not, their precomputed KV caches cannot be directly used since they ignore the text’s cross- attention with the preceding text in the LLM input. Thus, the benefits of reusing KV caches remain largely unrealized. &lt;/p&gt;
&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt; This project tackles just one question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same genera- tion quality as the expensive full prefill (i.e., without reusing KV cache)? We present CacheFuse, a scheme that reuses the pre-computed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheFuse to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheFuse with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular bench- mark datasets of different tasks, we show that CacheFuse reduces time-to-first-token (TTFT) by 2.2–3.3× and increases the inference throughput by 2.8-5×, compared with full KV recompute, without compromising generation quality or incurring more storage cost. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kuntai Du starts his internship at Sky Computing Lab at Berkeley.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/kuntai_sky/</link>
      <pubDate>Fri, 10 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/kuntai_sky/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; The &lt;a href=&#34;https://sky.cs.berkeley.edu/&#34; &gt;Sky Computing Lab &lt;/a&gt; represents the next chapter of data-intensive systems research at Berkeley. Recent years have seen the explosion of cloud computing. Applications are moving their data and computation to the cloud; on-premise services are dying. In doing so, companies have to make difficult choices between the myriad of cloud providers, each with different services or hardware. Lock-in, whether through artificial migration costs, legal constraints or engineering baggage is real. In the Sky Computing Lab, we will leverage distributed systems, programming languages, security, and machine learning to decouple the services that a company wants to implement from the choice of a specific cloud.&lt;/p&gt;
&lt;p&gt;Congratulations Kuntai!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving accepted at SIGCOMM&#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/cachegen_sigcomm/</link>
      <pubDate>Tue, 07 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/cachegen_sigcomm/</guid>
      <description>&lt;p&gt;Congratulations to Yuhan et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Resource allocation for Multi-Tenant Retrieval Augmented Generation Systems 
</title>
      <link>https://uchi-jcl.github.io/group-website/projects/ragoptimization/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/projects/ragoptimization/</guid>
      <description>&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt; Retrieval Augmented Generation (RAG) is the recent state-of-the art paradigm which lets Large Language Models (LLMs) generalise to text generation tasks in new domains, for which they have not been trained. Traditionally, this generalisation was achieved by fine-tuning the LLM on a small amount of in-domain data, which is slow and expensive, as compared to RAG.&lt;/p&gt;
&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt; RAG today focuses on two key ideas information retrieval or how to fetch the relevant chunks to answer the LLM query and synthesis or how to combine the chunks for the LLM to process efficiently. In this paper, we show how the choice of configurations can have a significant trade-off between generation quality and system resource utilization in the synthesis section of the RAG pipeline. We conduct a preliminary measurement study to show how different configurations can yield the an optimal method for combining the retrieved documents to obtain the best performance in terms of output quality and output latency, while also measuring the effect of the trade-offs in choosing one dimension over the other. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Automatic and Efficient Customization of Neural Networks for ML Applications</title>
      <link>https://uchi-jcl.github.io/group-website/publication/automl/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/automl/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>GRACE: Loss-Resilient Real-Time Video through Neural Codecs</title>
      <link>https://uchi-jcl.github.io/group-website/publication/grace/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/grace/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Knowledge Streaming from LLMs to Environments
</title>
      <link>https://uchi-jcl.github.io/group-website/projects/knowledgestreaming/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/projects/knowledgestreaming/</guid>
      <description>&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt; Nowadays, not only can we chat with chatGPT, but we can also let it write and execute code to do more complex tasks (e.g. letting chatGPT plot the boundary of America using Python). However, it takes tens of seconds for chatGPT to finish generating the code, executing the code, and showing the final result. We aim to make this process faster, by executing a line of code right after it is generated. This simple trick can make chatGPT show the code execution results 1.5x faster on a wide range of applications!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Earth&#43;: on-board satellite imagery compression leveraging historical earth observations</title>
      <link>https://uchi-jcl.github.io/group-website/publication/earthplus/</link>
      <pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/earthplus/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Grace: Loss-Resilient Real-Time Video through Neural Codecs accepted at NSDI&#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/grace_nsdi/</link>
      <pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/grace_nsdi/</guid>
      <description>&lt;p&gt;Congratulations to Yihua Cheng et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving
</title>
      <link>https://uchi-jcl.github.io/group-website/projects/cachegen/</link>
      <pubDate>Wed, 31 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/projects/cachegen/</guid>
      <description>&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt; As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge or user-specific information. Yet using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause extra network delays. &lt;/p&gt;
&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt; CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, which embraces KV cache&#39;s distributional properties, to encode a KV cache into more compact bitstream representations with negligible encoding/decoding overhead. This reduces the bandwidth demand to fetch the KV cache. Second, to maintain low context-loading delay and high generation quality, CacheGen adapts the streaming strategies to cope with changes in available bandwidth. When available bandwidth drops, CacheGen may raise the compression level for a part of the context or choose to recompute its KV cache on the fly. We test CacheGen on four popular LLMs of various sizes and four datasets (662 contexts in total). Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3.2x while having negligible impact on the LLM response quality in accuracy or perplexity. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Earth&#43;: on-board satellite imagery compression leveraging historical earth observations
</title>
      <link>https://uchi-jcl.github.io/group-website/projects/earthplus/</link>
      <pubDate>Wed, 31 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/projects/earthplus/</guid>
      <description>&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt; Satellite imagery is useful for a wide range of applications, from automatic road detection to forest monitoring. But did you know only 2% of satellite images can actually be downloaded to the ground? To help satellites download more images, we&#39;ve noticed that the same locations is frequently captured by different satellites and the captured images can be quite similar. By using this similarity, we can squeeze satellite images down to 3x smaller. &lt;/p&gt;
&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt;
With the increasing deployment of earth observation satellite constellations, the downlink (satellite-to-ground) capacity often limits the freshness, quality, and coverage of the imagery data available to applications on the ground. To overcome the downlink limitation, we present Earth+, a new satellite imagery compression system that, instead of compressing each image individually, pinpoints and downloads only recent imagery changes with respect to the history reference images. To minimize the amount of changes, it is critical to make reference images as fresh as possible. Earth+ enables each satellite to choose fresh reference images from not only its own history images but also past images of other satellites from an entire satellite constellation. To share reference images across satellites, Earth+ utilizes the limited capacity of the existing uplink (ground-to-satellite) by judiciously selecting and compressing reference images while still allowing accurate change detection. In short, Earth+ is the first to make reference-based compression efficient, by enabling constellation-wide sharing of fresh reference images across satellites. Our evaluation shows that Earth+ can reduce the downlink usage by a factor of 3.3 compared to state-of-the-art on-board image compression techniques while not sacrificing image quality, or using more on-board computing or storage resources, or more uplink bandwidth than currently available. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Automatic and Efficient Customization of Neural Networks for ML Applications accecpted at OSDI&#39;24.</title>
      <link>https://uchi-jcl.github.io/group-website/post/automl_osdi/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/automl_osdi/</guid>
      <description>&lt;p&gt;Congratulations to Yuhan et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GRACE: Loss-Resilient Real-Time Video through Neural Codecs
</title>
      <link>https://uchi-jcl.github.io/group-website/projects/grace/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/projects/grace/</guid>
      <description>&lt;p style=&#34;font-size:18px&#34; align=&#34;justify&#34;&gt; In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder. We present a loss-resilient real-time video system called GRACE, which preserves the user&#39;s quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE&#39;s enhanced loss resilience is its joint training of the neural encoder and decoder under a spectrum of simulated packet losses. In lossless scenarios, GRACE achieves video quality on par with conventional codecs (e.g., H.265). As the loss rate escalates, GRACE exhibits a more graceful, less pronounced decline in quality, consistently outperforming other loss-resilient schemes. Through extensive evaluation on various videos and real network traces, we demonstrate that GRACE reduces undecodable frames by 95% and stall duration by 90% compared with FEC, while markedly boosting video quality over error concealment methods. In a user study with 240 crowdsourced participants and 960 subjective ratings, GRACE registers a 38% higher mean opinion score (MOS) than other baselines. &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation accepted at SoCC&#39;23.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/one_adapt_socc/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/one_adapt_socc/</guid>
      <description>&lt;p&gt;Congratulations to Kuntai et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Online Profiling and Adaptation of Quality Sensitivity for Internet Video accepted at SoCC&#39;23.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/online_prof_socc/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/online_prof_socc/</guid>
      <description>&lt;p&gt;Congratulations to Yihua et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>VidPlat: A Tool for Fast Crowdsourcing of Quality-of-Experience Measurements</title>
      <link>https://uchi-jcl.github.io/group-website/publication/vidplat/</link>
      <pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/vidplat/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Enabling Perception-Driven Optimization in Networking</title>
      <link>https://uchi-jcl.github.io/group-website/publication/perception/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/perception/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Kuntai Du named as Siebel Scholar for the class of &#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/kuntai_siebel/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/kuntai_siebel/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; The Siebel Scholars program awards grants to 16 universities in the United States, China, France, Italy and Japan. Following a competitive review process by the deans of their respective schools on the basis of outstanding academic achievement and demonstrated leadership, the top graduate students from 27 partner programs are selected each year as Siebel Scholars and receive a $35,000 award for their final year of studies. On average, Siebel Scholars rank in the top five percent of their class, many within the top one percent. &lt;/p&gt;
&lt;p&gt;Congratulations Kuntai!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation</title>
      <link>https://uchi-jcl.github.io/group-website/publication/oneadapt/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/oneadapt/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Online Profiling and Adaptation of Quality Sensitivity for Internet Video</title>
      <link>https://uchi-jcl.github.io/group-website/publication/onlineprofile/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/onlineprofile/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Run-Time Prevention of Software Integration Failures of Machine Learning APIs</title>
      <link>https://uchi-jcl.github.io/group-website/publication/runtimeml/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/runtimeml/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Raising the Level of Abstraction for Time-State Analytics With the Timeline Framework</title>
      <link>https://uchi-jcl.github.io/group-website/publication/timeline/</link>
      <pubDate>Mon, 02 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/timeline/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Contact Us</title>
      <link>https://uchi-jcl.github.io/group-website/contact/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>People</title>
      <link>https://uchi-jcl.github.io/group-website/people/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/people/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AccMPEG: Optimizing Video Encoding for Video Analytics</title>
      <link>https://uchi-jcl.github.io/group-website/publication/accmpeg/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/accmpeg/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>Automatic Curriculum Generation for Learning Adaptation in Networking</title>
      <link>https://uchi-jcl.github.io/group-website/publication/genet/</link>
      <pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/genet/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>SENSEI: Aligning Video Streaming Quality with Dynamic User Sensitivity</title>
      <link>https://uchi-jcl.github.io/group-website/publication/sensei/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/publication/sensei/</guid>
      <description>&lt;!-- 

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;




&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.
  &lt;/div&gt;
&lt;/div&gt;


Add the publication&#39;s **full text** or **supplementary notes** here. You can use rich formatting such as including [code, math, and images](https://docs.hugoblox.com/content/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://uchi-jcl.github.io/group-website/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
