<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest News | Junchen&#39;s Lab</title>
    <link>https://uchi-jcl.github.io/group-website/post/</link>
      <atom:link href="https://uchi-jcl.github.io/group-website/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest News</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 04 Aug 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://uchi-jcl.github.io/group-website/media/icon_hua25ef39b30b8f32ff2a6fd2e1eed54f5_1032324_512x512_fill_lanczos_center_3.png</url>
      <title>Latest News</title>
      <link>https://uchi-jcl.github.io/group-website/post/</link>
    </image>
    
    <item>
      <title>Hanchen presented CacheGen paper at SIGCOMM&#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/hanchen_sigcomm/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/hanchen_sigcomm/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; CacheGen is a LLM KV cache compression and streaming system that saves storage size and reduces transfer time. By utilizing delta compression and information theory, CacheGen is able to further compress KV cache by up to 4.3x on top of previous machine learning techniques with option to incur NO additional loss. &lt;/p&gt;
&lt;p&gt;Link to a pre-recorded version:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/H4_OUWvdiNo?si=oWtNqMszSVOqKeAV&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>Junchen&#39;s talk on Knowledge-Delivery Networks at ByteDance and APNet&#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/junchen_bytedance/</link>
      <pubDate>Sun, 04 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/junchen_bytedance/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; Imagine once an LLM has processed a document, the &#34;knowledge&#34; can be instantly shared with other LLM instances. Unfortunately, today, LLMs must read the same long document multiple times, causing a significant slowdown. We introduce a new Knowledge Delivery Network that enables LLMs to efficiently share their digested knowledge, in the form of KV caches, so only one LLM instance needs to process (prefill) each document. The key challenge is how to store the KV caches cheaply and serve them quickly. Instead of keeping the KV caches of all reusable chunks in GPU or CPU memory, we show that with careful design and implementation, storing them on cheaper devices is not only economically superior but also delivers significant reductions in LLM serving delay, especially the time to the first token. &lt;/p&gt;
&lt;br&gt; 
Slides link : &lt;a href=&#34;https://people.cs.uchicago.edu/~junchenj/docs/LLM_Jiang_v10_APNet.pptx&#34;&gt; Download &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Yuhan Liu presented ChameleonAPI at OSDI&#39;24.</title>
      <link>https://uchi-jcl.github.io/group-website/post/yuhan_osdi/</link>
      <pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/yuhan_osdi/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; ChameleonAPI provides application developers with a parser that automatically analyzes the application to produce an abstract of its decision process, which is then used to devise an application-specific loss function that only penalizes API output errors critical to the application. ChameleonAPI uses the loss function to efficiently train a neural network model customized for each application and deploys it to serve API invocations from the respective application via existing interface. Compared to a baseline that selects the best-of-all commercial ML API, ChameleonAPI reduces incorrect application decisions by 43%. &lt;/p&gt;
&lt;br&gt; 
Check out the paper for more details : &lt;a href=&#34;https://www.usenix.org/conference/osdi24/presentation/liu&#34;&gt; ChameleonAPI &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Zhengxu Xia successfully completes his PhD candidacy exam at UChicago.</title>
      <link>https://uchi-jcl.github.io/group-website/post/zhengxu_cand/</link>
      <pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/zhengxu_cand/</guid>
      <description>&lt;p&gt;Congratulations Zhengxu!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Yihua Cheng starts his internship at Conviva.</title>
      <link>https://uchi-jcl.github.io/group-website/post/yihua_conv/</link>
      <pubDate>Tue, 04 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/yihua_conv/</guid>
      <description>&lt;p&gt;Congratulations Yihua!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion out on ArXiv.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/cacheblend/</link>
      <pubDate>Mon, 03 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/cacheblend/</guid>
      <description>&lt;p&gt;Check out the paper for more details : &lt;a href=&#34;https://arxiv.org/abs/2405.16444&#34;&gt; Paper &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Yuhan Liu starts her internship at Microsoft Research.</title>
      <link>https://uchi-jcl.github.io/group-website/post/yuhan_msr/</link>
      <pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/yuhan_msr/</guid>
      <description>&lt;p&gt;Congratulations Yuhan!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Yuhan Liu successfully completes her MS exam at UChicago.</title>
      <link>https://uchi-jcl.github.io/group-website/post/yuhan_master/</link>
      <pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/yuhan_master/</guid>
      <description>&lt;p&gt;Congratulations Yuhan!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Junchen&#39;s talk on the LLM Caching Layer at Databricks and Alluxio’s AI/ML Infra meetup.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/junchen_databricks/</link>
      <pubDate>Wed, 22 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/junchen_databricks/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; Prefill in LLM inference is known to be resource-intensive, especially for long LLM inputs. While better scheduling can mitigate prefill’s impact, it would be fundamentally better to avoid (most of) prefill. This talk introduces our preliminary effort towards drastically minimizing prefill delay for LLM inputs that naturally reuse text chunks, such as in retrieval-augmented generation. While keeping the KV cache of all text chunks in memory is difficult, we show that it is possible to store them on cheaper yet slower storage. By improving the loading process of the reused KV caches, we can still significantly speed up prefill delay while maintaining the same generation quality. &lt;/.p&gt;
&lt;p&gt;Talk link :&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/a5YH2RCe4NU?si=7hRisB8JXXl19grx&#34; title=&#34;YouTube video player&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Slides link :&lt;/p&gt;
&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/eOSyDpGElSFWvB&#34; width=&#34;560&#34; height=&#34;315&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt; &lt;div style=&#34;margin-bottom:5px&#34;&gt; &lt;strong&gt; &lt;a href=&#34;//www.slideshare.net/slideshow/ai-ml-infra-meetup-reducing-prefill-for-llm-serving-in-rag/269308079&#34; title=&#34;&#34; target=&#34;_blank&#34;&gt;AI/ML Infra Meetup | Reducing Prefill for LLM Serving in RAG&lt;/a&gt; &lt;/strong&gt; &lt;/strong&gt; &lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kuntai Du starts his internship at Sky Computing Lab at Berkeley.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/kuntai_sky/</link>
      <pubDate>Fri, 10 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/kuntai_sky/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; The &lt;a href=&#34;https://sky.cs.berkeley.edu/&#34; &gt;Sky Computing Lab &lt;/a&gt; represents the next chapter of data-intensive systems research at Berkeley. Recent years have seen the explosion of cloud computing. Applications are moving their data and computation to the cloud; on-premise services are dying. In doing so, companies have to make difficult choices between the myriad of cloud providers, each with different services or hardware. Lock-in, whether through artificial migration costs, legal constraints or engineering baggage is real. In the Sky Computing Lab, we will leverage distributed systems, programming languages, security, and machine learning to decouple the services that a company wants to implement from the choice of a specific cloud.&lt;/p&gt;
&lt;p&gt;Congratulations Kuntai!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving accepted at SIGCOMM&#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/cachegen_sigcomm/</link>
      <pubDate>Tue, 07 May 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/cachegen_sigcomm/</guid>
      <description>&lt;p&gt;Congratulations to Yuhan et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Grace: Loss-Resilient Real-Time Video through Neural Codecs accepted at NSDI&#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/grace_nsdi/</link>
      <pubDate>Fri, 15 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/grace_nsdi/</guid>
      <description>&lt;p&gt;Congratulations to Yihua Cheng et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Automatic and Efficient Customization of Neural Networks for ML Applications accecpted at OSDI&#39;24.</title>
      <link>https://uchi-jcl.github.io/group-website/post/automl_osdi/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/automl_osdi/</guid>
      <description>&lt;p&gt;Congratulations to Yuhan et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation accepted at SoCC&#39;23.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/one_adapt_socc/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/one_adapt_socc/</guid>
      <description>&lt;p&gt;Congratulations to Kuntai et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Online Profiling and Adaptation of Quality Sensitivity for Internet Video accepted at SoCC&#39;23.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/online_prof_socc/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/online_prof_socc/</guid>
      <description>&lt;p&gt;Congratulations to Yihua et al.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Kuntai Du named as Siebel Scholar for the class of &#39;24.
</title>
      <link>https://uchi-jcl.github.io/group-website/post/kuntai_siebel/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://uchi-jcl.github.io/group-website/post/kuntai_siebel/</guid>
      <description>&lt;p align=&#34;justify&#34;&gt; The Siebel Scholars program awards grants to 16 universities in the United States, China, France, Italy and Japan. Following a competitive review process by the deans of their respective schools on the basis of outstanding academic achievement and demonstrated leadership, the top graduate students from 27 partner programs are selected each year as Siebel Scholars and receive a $35,000 award for their final year of studies. On average, Siebel Scholars rank in the top five percent of their class, many within the top one percent. &lt;/p&gt;
&lt;p&gt;Congratulations Kuntai!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
