
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hanchen Li is a third-year undergraduate at University of Chicago working on System and Networking for AI/Multimedia. In his free time, he enjoys playing soccer, basketball and reading social sciences.\n","date":1717372800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717372800,"objectID":"e9d62c71c41ee5072d516c8fb4159a7c","permalink":"https://uchi-jcl.github.io/group-website/author/hanchen-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/hanchen-li/","section":"authors","summary":"Hanchen Li is a third-year undergraduate at University of Chicago working on System and Networking for AI/Multimedia. In his free time, he enjoys playing soccer, basketball and reading social sciences.","tags":null,"title":"Hanchen Li","type":"authors"},{"authors":null,"categories":null,"content":"I’m a fourth-year undergraduate at The Chinese University of Hong Kong, Shenzhen. I will become a PHD student at University of Chicago this fall, co-advised by Prof. Junchen Jiang and Prof. Shan Lu. My research interest lies in the synergy between machine learning and computer systems.\n","date":1717372800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717372800,"objectID":"9bdd9871fcba60acee8b1a302f6cc727","permalink":"https://uchi-jcl.github.io/group-website/author/jiayi-yao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/jiayi-yao/","section":"authors","summary":"I’m a fourth-year undergraduate at The Chinese University of Hong Kong, Shenzhen. I will become a PHD student at University of Chicago this fall, co-advised by Prof. Junchen Jiang and Prof.","tags":null,"title":"Jiayi Yao","type":"authors"},{"authors":null,"categories":null,"content":"Welcome! I joined the Dept. of Computer Science at the University of Chicago as an Assistant Professor in July 2018.\nI received my B.A. in computer science from Tsinghua University (Yao Class). I received my PhD in computer science from Carnegie Mellon University in 2017. I was advised by Vyas Sekar and Hui Zhang.\nResearch interests: networked systems. My research applies state-of-the-art machine learning techniques to drastically improve the performance and reliability of large-scale networked systems.\n","date":1717372800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717372800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://uchi-jcl.github.io/group-website/author/junchen-jiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/junchen-jiang/","section":"authors","summary":"Welcome! I joined the Dept. of Computer Science at the University of Chicago as an Assistant Professor in July 2018.\nI received my B.A. in computer science from Tsinghua University (Yao Class).","tags":null,"title":"Junchen Jiang","type":"authors"},{"authors":null,"categories":null,"content":"I am Kuntai Du, a rising 5th-year CS Ph.D. student in University of Chicago, advised by Junchen Jiang . My current research interest is video analytics. I got my bachelor degree in Peking University.\n","date":1717372800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717372800,"objectID":"ee7704841cbaeba27b87cfe50762b9c4","permalink":"https://uchi-jcl.github.io/group-website/author/kuntai-du/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/kuntai-du/","section":"authors","summary":"I am Kuntai Du, a rising 5th-year CS Ph.D. student in University of Chicago, advised by Junchen Jiang . My current research interest is video analytics. I got my bachelor degree in Peking University.","tags":null,"title":"Kuntai Du","type":"authors"},{"authors":null,"categories":null,"content":"I am a first year PhD student in Computer Science at the University of Chicago, advised by Junchen Jiang and Nick Feamster. I am broadly interested in machine learning methods for performance improvement in computer networks.\n","date":1717372800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717372800,"objectID":"8ae1b1f0b984cbf9774246cf9593f816","permalink":"https://uchi-jcl.github.io/group-website/author/siddhant-ray/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/siddhant-ray/","section":"authors","summary":"I am a first year PhD student in Computer Science at the University of Chicago, advised by Junchen Jiang and Nick Feamster. I am broadly interested in machine learning methods for performance improvement in computer networks.","tags":null,"title":"Siddhant Ray","type":"authors"},{"authors":null,"categories":null,"content":"Hi, There! This is Yihua Cheng, a rising 4th-year CS Ph.D. candidate in University of Chicago, advised by Junchen Jiang . My research is focusing on computer networks and systems. Specifically, I’m interested in real-time video streaming systems and high-performance data-processing systems. I got my bachelor degree in Peking University in 2020.\n","date":1717372800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717372800,"objectID":"73663b1580fabb326c295e85f8aa2aab","permalink":"https://uchi-jcl.github.io/group-website/author/yihua-cheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/yihua-cheng/","section":"authors","summary":"Hi, There! This is Yihua Cheng, a rising 4th-year CS Ph.D. candidate in University of Chicago, advised by Junchen Jiang . My research is focusing on computer networks and systems. Specifically, I’m interested in real-time video streaming systems and high-performance data-processing systems.","tags":null,"title":"Yihua Cheng","type":"authors"},{"authors":null,"categories":null,"content":"I’m a third-year PhD student in the Department of CS at University of Chicago, advised by Prof. Junchen Jiang and Prof. Shan Lu. My research interest is Systems/Software engineering for ML. I received my B.S. in CS at University of Wisconsin-Madison, fortunate to be advised by Prof. Shivaram Venkataraman.\n","date":1717372800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717372800,"objectID":"41f71acdc4ad98d21e8092774c53a47d","permalink":"https://uchi-jcl.github.io/group-website/author/yuhan-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/yuhan-liu/","section":"authors","summary":"I’m a third-year PhD student in the Department of CS at University of Chicago, advised by Prof. Junchen Jiang and Prof. Shan Lu. My research interest is Systems/Software engineering for ML.","tags":null,"title":"Yuhan Liu","type":"authors"},{"authors":null,"categories":null,"content":"","date":1711929600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1711929600,"objectID":"be2ea6246309e1c622e9ccb0a8fbb14d","permalink":"https://uchi-jcl.github.io/group-website/author/xu-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/xu-zhang/","section":"authors","summary":"","tags":null,"title":"Xu Zhang","type":"authors"},{"authors":null,"categories":null,"content":"I’m a fifth-year PhD student working on reinforcement learning for networking and video analytics.\n","date":1661990400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1661990400,"objectID":"3291feb0ffc9dc5ce7155ba5c2befff0","permalink":"https://uchi-jcl.github.io/group-website/author/zhengxu-xia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/zhengxu-xia/","section":"authors","summary":"I’m a fifth-year PhD student working on reinforcement learning for networking and video analytics.","tags":null,"title":"Zhengxu Xia","type":"authors"},{"authors":null,"categories":null,"content":"I am a third-year undergraduate student at the University of Chicago majoring in Mathematics (with Specialization in Economics) and Computer Science. My research interest lies broadly in Mathematics, Networked Systems and Machine Learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ffde99b08b6aed75b4e1da1237f4b2f3","permalink":"https://uchi-jcl.github.io/group-website/author/zhuohan-gu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/group-website/author/zhuohan-gu/","section":"authors","summary":"I am a third-year undergraduate student at the University of Chicago majoring in Mathematics (with Specialization in Economics) and Computer Science. My research interest lies broadly in Mathematics, Networked Systems and Machine Learning.","tags":null,"title":"Zhuohan Gu","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://uchi-jcl.github.io/group-website/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/group-website/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":null,"categories":null,"content":"Congratulations Zhengxu!\n","date":1717977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717977600,"objectID":"83170ab7910a599d509841bf751b0b4b","permalink":"https://uchi-jcl.github.io/group-website/post/zhengxu_cand/","publishdate":"2024-06-10T00:00:00Z","relpermalink":"/group-website/post/zhengxu_cand/","section":"post","summary":"Congratulations Zhengxu!\n","tags":null,"title":"Zhengxu Xia successfully completes his PhD candidacy exam at UChicago.","type":"post"},{"authors":null,"categories":null,"content":"Congratulations Yihua!\n","date":1717459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717459200,"objectID":"90db5c503a86b09581ef2b8edf503bf9","permalink":"https://uchi-jcl.github.io/group-website/post/yihua_conv/","publishdate":"2024-06-04T00:00:00Z","relpermalink":"/group-website/post/yihua_conv/","section":"post","summary":"Congratulations Yihua!\n","tags":null,"title":"Yihua Cheng starts his internship at Conviva.","type":"post"},{"authors":["Jiayi Yao","Hanchen Li","Yuhan Liu","Siddhant Ray","Yihua Cheng","Qizheng Zhang","Kuntai Du","Shan Lu","Junchen Jiang"],"categories":null,"content":" ","date":1717372800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717372800,"objectID":"4c7df8461033998317ba6ef22fb6ec8b","permalink":"https://uchi-jcl.github.io/group-website/publication/cacheblend/","publishdate":"2024-06-03T00:00:00Z","relpermalink":"/group-website/publication/cacheblend/","section":"publication","summary":"Partical KV Cache recompute for fast LLM interence and serving.","tags":null,"title":"CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion","type":"publication"},{"authors":null,"categories":null,"content":"Check out the paper for more details : Paper ","date":1717372800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717372800,"objectID":"6e988cb72504b31a915dd67b482030e0","permalink":"https://uchi-jcl.github.io/group-website/post/cacheblend/","publishdate":"2024-06-03T00:00:00Z","relpermalink":"/group-website/post/cacheblend/","section":"post","summary":"Check out the paper for more details : Paper ","tags":null,"title":"CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion out on ArXiv.\n","type":"post"},{"authors":null,"categories":null,"content":"Congratulations Yuhan!\n","date":1716854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716854400,"objectID":"61a6105998b481b58001d65c80ddde6a","permalink":"https://uchi-jcl.github.io/group-website/post/yuhan_msr/","publishdate":"2024-05-28T00:00:00Z","relpermalink":"/group-website/post/yuhan_msr/","section":"post","summary":"Congratulations Yuhan!\n","tags":null,"title":"Yuhan Liu starts her internship at Microsoft Research.","type":"post"},{"authors":null,"categories":null,"content":"Congratulations Yuhan!\n","date":1716854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716854400,"objectID":"134ca5128051d0f3159556a58ce7897c","permalink":"https://uchi-jcl.github.io/group-website/post/yuhan_master/","publishdate":"2024-05-28T00:00:00Z","relpermalink":"/group-website/post/yuhan_master/","section":"post","summary":"Congratulations Yuhan!\n","tags":null,"title":"Yuhan Liu successfully completes her MS exam at UChicago.","type":"post"},{"authors":null,"categories":null,"content":" Prefill in LLM inference is known to be resource-intensive, especially for long LLM inputs. While better scheduling can mitigate prefill’s impact, it would be fundamentally better to avoid (most of) prefill. This talk introduces our preliminary effort towards drastically minimizing prefill delay for LLM inputs that naturally reuse text chunks, such as in retrieval-augmented generation. While keeping the KV cache of all text chunks in memory is difficult, we show that it is possible to store them on cheaper yet slower storage. By improving the loading process of the reused KV caches, we can still significantly speed up prefill delay while maintaining the same generation quality. \u0026lt;/.p\u0026gt; Talk link :\nSlides link :\nAI/ML Infra Meetup | Reducing Prefill for LLM Serving in RAG ","date":1716336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716336000,"objectID":"650226bc0e037496c4e91af613bb222c","permalink":"https://uchi-jcl.github.io/group-website/post/junchen_databricks/","publishdate":"2024-05-22T00:00:00Z","relpermalink":"/group-website/post/junchen_databricks/","section":"post","summary":" Prefill in LLM inference is known to be resource-intensive, especially for long LLM inputs. While better scheduling can mitigate prefill’s impact, it would be fundamentally better to avoid (most of) prefill. This talk introduces our preliminary effort towards drastically minimizing prefill delay for LLM inputs that naturally reuse text chunks, such as in retrieval-augmented generation. While keeping the KV cache of all text chunks in memory is difficult, we show that it is possible to store them on cheaper yet slower storage. By improving the loading process of the reused KV caches, we can still significantly speed up prefill delay while maintaining the same generation quality. \u003c/.p\u003e Talk link :\nSlides link :\nAI/ML Infra Meetup | Reducing Prefill for LLM Serving in RAG ","tags":null,"title":"Junchen's talk on the LLM Caching Layer at Databricks and Alluxio’s AI/ML Infra meetup.\n","type":"post"},{"authors":null,"categories":null,"content":" Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, and when they are not, their precomputed KV caches cannot be directly used since they ignore the text’s cross- attention with the preceding text in the LLM input. Thus, the benefits of reusing KV caches remain largely unrealized. This project tackles just one question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same genera- tion quality as the expensive full prefill (i.e., without reusing KV cache)? We present CacheFuse, a scheme that reuses the pre-computed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheFuse to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheFuse with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular bench- mark datasets of different tasks, we show that CacheFuse reduces time-to-first-token (TTFT) by 2.2–3.3× and increases the inference throughput by 2.8-5×, compared with full KV recompute, without compromising generation quality or incurring more storage cost. ","date":1716249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716249600,"objectID":"85faaf5f59fb190eb75c50f4ab4a4549","permalink":"https://uchi-jcl.github.io/group-website/projects/cachefuse/","publishdate":"2024-05-21T00:00:00Z","relpermalink":"/group-website/projects/cachefuse/","section":"projects","summary":"Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input.","tags":null,"title":"Using Cached Knowledge for Large Language Model Serving with CacheFuse\n","type":"projects"},{"authors":null,"categories":null,"content":" The Sky Computing Lab represents the next chapter of data-intensive systems research at Berkeley. Recent years have seen the explosion of cloud computing. Applications are moving their data and computation to the cloud; on-premise services are dying. In doing so, companies have to make difficult choices between the myriad of cloud providers, each with different services or hardware. Lock-in, whether through artificial migration costs, legal constraints or engineering baggage is real. In the Sky Computing Lab, we will leverage distributed systems, programming languages, security, and machine learning to decouple the services that a company wants to implement from the choice of a specific cloud.\nCongratulations Kuntai!\n","date":1715299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715299200,"objectID":"a4a6ef72b85d5e87d14ae172ed153b56","permalink":"https://uchi-jcl.github.io/group-website/post/kuntai_sky/","publishdate":"2024-05-10T00:00:00Z","relpermalink":"/group-website/post/kuntai_sky/","section":"post","summary":" The Sky Computing Lab represents the next chapter of data-intensive systems research at Berkeley. Recent years have seen the explosion of cloud computing. Applications are moving their data and computation to the cloud; on-premise services are dying. In doing so, companies have to make difficult choices between the myriad of cloud providers, each with different services or hardware. Lock-in, whether through artificial migration costs, legal constraints or engineering baggage is real. In the Sky Computing Lab, we will leverage distributed systems, programming languages, security, and machine learning to decouple the services that a company wants to implement from the choice of a specific cloud.\nCongratulations Kuntai!\n","tags":null,"title":"Kuntai Du starts his internship at Sky Computing Lab at Berkeley.\n","type":"post"},{"authors":null,"categories":null,"content":"Congratulations to Yuhan et al.\n","date":1715040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715040000,"objectID":"627a14e8a5f9fb1aee10fd26cb71f20d","permalink":"https://uchi-jcl.github.io/group-website/post/cachegen_sigcomm/","publishdate":"2024-05-07T00:00:00Z","relpermalink":"/group-website/post/cachegen_sigcomm/","section":"post","summary":"Congratulations to Yuhan et al.\n","tags":null,"title":"CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving accepted at SIGCOMM'24.\n","type":"post"},{"authors":null,"categories":null,"content":" Retrieval Augmented Generation (RAG) is the recent state-of-the art paradigm which lets Large Language Models (LLMs) generalise to text generation tasks in new domains, for which they have not been trained. Traditionally, this generalisation was achieved by fine-tuning the LLM on a small amount of in-domain data, which is slow and expensive, as compared to RAG.\nRAG today focuses on two key ideas information retrieval or how to fetch the relevant chunks to answer the LLM query and synthesis or how to combine the chunks for the LLM to process efficiently. In this paper, we show how the choice of configurations can have a significant trade-off between generation quality and system resource utilization in the synthesis section of the RAG pipeline. We conduct a preliminary measurement study to show how different configurations can yield the an optimal method for combining the retrieved documents to obtain the best performance in terms of output quality and output latency, while also measuring the effect of the trade-offs in choosing one dimension over the other. ","date":1714521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714521600,"objectID":"0579326ccbbe6e5110800a398a1d97ad","permalink":"https://uchi-jcl.github.io/group-website/projects/ragoptimization/","publishdate":"2024-05-01T00:00:00Z","relpermalink":"/group-website/projects/ragoptimization/","section":"projects","summary":"Retrieval Augmented Generation (RAG) is the recent state-of-the art paradigm which lets Large Language Models (LLMs) generalise to text generation tasks in new domains, for which they have not been trained.","tags":null,"title":"Resource allocation for Multi-Tenant Retrieval Augmented Generation Systems \n","type":"projects"},{"authors":["Yuhan Liu","Chengcheng Wan","Kuntai Du","Anton Arapin","Henry Hoffmann","Qizheng Zhang","Junchen Jiang","Shan Lu","Michael Maire"],"categories":null,"content":" ","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"27ea5033739421acaadfddbe5d7cba21","permalink":"https://uchi-jcl.github.io/group-website/publication/automl/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/group-website/publication/automl/","section":"publication","summary":"Trained neural network to reduce incorrect application decisions.","tags":[],"title":"Automatic and Efficient Customization of Neural Networks for ML Applications","type":"publication"},{"authors":["Yihua Cheng","Ziyi Zhang","Hanchen Li","Anton Arapin","Yue Zhang","Qizheng Zhang","Yuhan Liu","Kuntai Du","Xu Zhang","Francis Y. Yan","Amrita Mazumdar","Nick Feamster","Junchen Jiang"],"categories":null,"content":" ","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"40356a6987e6b4ef847bf08cb816772c","permalink":"https://uchi-jcl.github.io/group-website/publication/grace/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/group-website/publication/grace/","section":"publication","summary":"Scalable neural video codec for loss resilience","tags":[],"title":"GRACE: Loss-Resilient Real-Time Video through Neural Codecs","type":"publication"},{"authors":null,"categories":null,"content":" Nowadays, not only can we chat with chatGPT, but we can also let it write and execute code to do more complex tasks (e.g. letting chatGPT plot the boundary of America using Python). However, it takes tens of seconds for chatGPT to finish generating the code, executing the code, and showing the final result. We aim to make this process faster, by executing a line of code right after it is generated. This simple trick can make chatGPT show the code execution results 1.5x faster on a wide range of applications!\n","date":1711929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711929600,"objectID":"54b0a62c98108383d13a0c4110e18ab5","permalink":"https://uchi-jcl.github.io/group-website/projects/knowledgestreaming/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/group-website/projects/knowledgestreaming/","section":"projects","summary":"Nowadays, not only can we chat with chatGPT, but we can also let it write and execute code to do more complex tasks (e.g. letting chatGPT plot the boundary of America using Python).","tags":null,"title":"Knowledge Streaming from LLMs to Environments\n","type":"projects"},{"authors":["Yuhan Liu","Hanchen Li","Yihua Cheng","Siddhant Ray","Yuyang Huang","Qizheng Zhang","Kuntai Du","Jiayi Yao","Shan Lu","Ganesh Ananthanarayanan","Michael Maire","Henry Hoffmann","Ari Holtzman","Junchen Jiang"],"categories":null,"content":" ","date":1711324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711324800,"objectID":"ddb5b6d9a3c21dcbfa22c809812b60d4","permalink":"https://uchi-jcl.github.io/group-website/publication/cachegen/","publishdate":"2024-03-25T00:00:00Z","relpermalink":"/group-website/publication/cachegen/","section":"publication","summary":"Compressed KV Cache loading for fast LLM interence and serving.","tags":null,"title":"CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving","type":"publication"},{"authors":["Kuntai Du","Yihua Cheng","Peder Olsen","Shadi Noghabi","Ranveer Chandra","Junchen Jiang"],"categories":null,"content":" ","date":1710720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710720000,"objectID":"d1c0866ccd847b57145961d65970c245","permalink":"https://uchi-jcl.github.io/group-website/publication/earthplus/","publishdate":"2024-03-18T00:00:00Z","relpermalink":"/group-website/publication/earthplus/","section":"publication","summary":"Selective compressing reference images while still allowing accurate change detection.","tags":null,"title":"Earth+: on-board satellite imagery compression leveraging historical earth observations","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations to Yihua Cheng et al.\n","date":1710460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710460800,"objectID":"522efc337b5bed384f64ce426d1fc50b","permalink":"https://uchi-jcl.github.io/group-website/post/grace_nsdi/","publishdate":"2024-03-15T00:00:00Z","relpermalink":"/group-website/post/grace_nsdi/","section":"post","summary":"Congratulations to Yihua Cheng et al.\n","tags":null,"title":"Grace: Loss-Resilient Real-Time Video through Neural Codecs accepted at NSDI'24.\n","type":"post"},{"authors":null,"categories":null,"content":" As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge or user-specific information. Yet using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause extra network delays. CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, which embraces KV cache\u0026#39;s distributional properties, to encode a KV cache into more compact bitstream representations with negligible encoding/decoding overhead. This reduces the bandwidth demand to fetch the KV cache. Second, to maintain low context-loading delay and high generation quality, CacheGen adapts the streaming strategies to cope with changes in available bandwidth. When available bandwidth drops, CacheGen may raise the compression level for a part of the context or choose to recompute its KV cache on the fly. We test CacheGen on four popular LLMs of various sizes and four datasets (662 contexts in total). Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3.2x while having negligible impact on the LLM response quality in accuracy or perplexity. ","date":1706659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706659200,"objectID":"60a0cad81353284587bdd491c248fcce","permalink":"https://uchi-jcl.github.io/group-website/projects/cachegen/","publishdate":"2024-01-31T00:00:00Z","relpermalink":"/group-website/projects/cachegen/","section":"projects","summary":"As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge or user-specific information. Yet using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until the whole context is processed by the LLM.","tags":null,"title":"CacheGen: KV Cache Compression and Streaming for Fast Language Model Serving\n","type":"projects"},{"authors":null,"categories":null,"content":" Satellite imagery is useful for a wide range of applications, from automatic road detection to forest monitoring. But did you know only 2% of satellite images can actually be downloaded to the ground? To help satellites download more images, we\u0026#39;ve noticed that the same locations is frequently captured by different satellites and the captured images can be quite similar. By using this similarity, we can squeeze satellite images down to 3x smaller. With the increasing deployment of earth observation satellite constellations, the downlink (satellite-to-ground) capacity often limits the freshness, quality, and coverage of the imagery data available to applications on the ground. To overcome the downlink limitation, we present Earth+, a new satellite imagery compression system that, instead of compressing each image individually, pinpoints and downloads only recent imagery changes with respect to the history reference images. To minimize the amount of changes, it is critical to make reference images as fresh as possible. Earth+ enables each satellite to choose fresh reference images from not only its own history images but also past images of other satellites from an entire satellite constellation. To share reference images across satellites, Earth+ utilizes the limited capacity of the existing uplink (ground-to-satellite) by judiciously selecting and compressing reference images while still allowing accurate change detection. In short, Earth+ is the first to make reference-based compression efficient, by enabling constellation-wide sharing of fresh reference images across satellites. Our evaluation shows that Earth+ can reduce the downlink usage by a factor of 3.3 compared to state-of-the-art on-board image compression techniques while not sacrificing image quality, or using more on-board computing or storage resources, or more uplink bandwidth than currently available. ","date":1706659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706659200,"objectID":"d7e008049375db81af1b5c10eac4bb56","permalink":"https://uchi-jcl.github.io/group-website/projects/earthplus/","publishdate":"2024-01-31T00:00:00Z","relpermalink":"/group-website/projects/earthplus/","section":"projects","summary":"Satellite imagery is useful for a wide range of applications, from automatic road detection to forest monitoring. But did you know only 2% of satellite images can actually be downloaded to the ground?","tags":null,"title":"Earth+: on-board satellite imagery compression leveraging historical earth observations\n","type":"projects"},{"authors":["Hanchen Li","Yuhan Liu","Yihua Cheng","Siddhant Ray","Kuntai Du","Junchen Jiang"],"categories":null,"content":" ","date":1705968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705968000,"objectID":"d737bdd6d057cc4ec6727e08541bd48e","permalink":"https://uchi-jcl.github.io/group-website/publication/chatterbox/","publishdate":"2024-01-23T00:00:00Z","relpermalink":"/group-website/publication/chatterbox/","section":"publication","summary":"Smart token serving for smoother LLM inference.","tags":null,"title":"Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network","type":"publication"},{"authors":null,"categories":null,"content":"Congratulations to Yuhan et al.\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"dce63dd457a62090ea617189bbc97227","permalink":"https://uchi-jcl.github.io/group-website/post/automl_osdi/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/group-website/post/automl_osdi/","section":"post","summary":"Congratulations to Yuhan et al.\n","tags":null,"title":"Automatic and Efficient Customization of Neural Networks for ML Applications accecpted at OSDI'24.","type":"post"},{"authors":null,"categories":null,"content":" In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder. We present a loss-resilient real-time video system called GRACE, which preserves the user\u0026#39;s quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE\u0026#39;s enhanced loss resilience is its joint training of the neural encoder and decoder under a spectrum of simulated packet losses. In lossless scenarios, GRACE achieves video quality on par with conventional codecs (e.g., H.265). As the loss rate escalates, GRACE exhibits a more graceful, less pronounced decline in quality, consistently outperforming other loss-resilient schemes. Through extensive evaluation on various videos and real network traces, we demonstrate that GRACE reduces undecodable frames by 95% and stall duration by 90% compared with FEC, while markedly boosting video quality over error concealment methods. In a user study with 240 crowdsourced participants and 960 subjective ratings, GRACE registers a 38% higher mean opinion score (MOS) than other baselines. ","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"2d0f608d1162c66c078270cda84876ee","permalink":"https://uchi-jcl.github.io/group-website/projects/grace/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/group-website/projects/grace/","section":"projects","summary":"In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment.","tags":null,"title":"GRACE: Loss-Resilient Real-Time Video through Neural Codecs\n","type":"projects"},{"authors":null,"categories":null,"content":"Congratulations to Kuntai et al.\n","date":1702598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702598400,"objectID":"5fba34c3462609821135bbd6c866e30c","permalink":"https://uchi-jcl.github.io/group-website/post/one_adapt_socc/","publishdate":"2023-12-15T00:00:00Z","relpermalink":"/group-website/post/one_adapt_socc/","section":"post","summary":"Congratulations to Kuntai et al.\n","tags":null,"title":"OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation accepted at SoCC'23.\n","type":"post"},{"authors":null,"categories":null,"content":"Congratulations to Yihua et al.\n","date":1702598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702598400,"objectID":"5fdd928889915ae80e5c3b9197e9c77d","permalink":"https://uchi-jcl.github.io/group-website/post/online_prof_socc/","publishdate":"2023-12-15T00:00:00Z","relpermalink":"/group-website/post/online_prof_socc/","section":"post","summary":"Congratulations to Yihua et al.\n","tags":null,"title":"Online Profiling and Adaptation of Quality Sensitivity for Internet Video accepted at SoCC'23.\n","type":"post"},{"authors":["Xu Zhang","Hanchen Li","Paul Schmitt","Marshini Chetty","Nick Feamster","Junchen Jiang"],"categories":null,"content":" ","date":1697587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697587200,"objectID":"7edc8b5cb0498bb1330652ad093b9ed5","permalink":"https://uchi-jcl.github.io/group-website/publication/vidplat/","publishdate":"2023-10-18T00:00:00Z","relpermalink":"/group-website/publication/vidplat/","section":"publication","summary":"Fast and automated QoE measurements by allowing dynamic pruning of QoE test videos within a single crowdsourcing task.","tags":null,"title":"VidPlat: A Tool for Fast Crowdsourcing of Quality-of-Experience Measurements","type":"publication"},{"authors":["Yihua Cheng","Xu Zhang","Junchen Jiang"],"categories":null,"content":" ","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"946514d100cbe3298fc8940f6a288cc6","permalink":"https://uchi-jcl.github.io/group-website/publication/perception/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/group-website/publication/perception/","section":"publication","summary":"Efficient collection of user feedback and enabling perception-driven optimization for Internet applications.","tags":[],"title":"Enabling Perception-Driven Optimization in Networking","type":"publication"},{"authors":null,"categories":null,"content":" The Siebel Scholars program awards grants to 16 universities in the United States, China, France, Italy and Japan. Following a competitive review process by the deans of their respective schools on the basis of outstanding academic achievement and demonstrated leadership, the top graduate students from 27 partner programs are selected each year as Siebel Scholars and receive a $35,000 award for their final year of studies. On average, Siebel Scholars rank in the top five percent of their class, many within the top one percent. Congratulations Kuntai!\n","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"13cadd404942eca155ab0ab0d76d14df","permalink":"https://uchi-jcl.github.io/group-website/post/kuntai_siebel/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/group-website/post/kuntai_siebel/","section":"post","summary":" The Siebel Scholars program awards grants to 16 universities in the United States, China, France, Italy and Japan. Following a competitive review process by the deans of their respective schools on the basis of outstanding academic achievement and demonstrated leadership, the top graduate students from 27 partner programs are selected each year as Siebel Scholars and receive a $35,000 award for their final year of studies. On average, Siebel Scholars rank in the top five percent of their class, many within the top one percent. Congratulations Kuntai!\n","tags":null,"title":"Kuntai Du named as Siebel Scholar for the class of '24.\n","type":"post"},{"authors":["Kuntai Du","Yuhan Liu","Yitian Hao","Qizheng Zhang","Haodong Wang","Yuyang Huang","Ganesh Ananthanarayanan","Junchen Jiang"],"categories":null,"content":" ","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"8cccd2eaae16fa3b4fbe28d2a725da2c","permalink":"https://uchi-jcl.github.io/group-website/publication/oneadapt/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/group-website/publication/oneadapt/","section":"publication","summary":"Gradient ascent training to adapt configuration knobs.","tags":[],"title":"OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation","type":"publication"},{"authors":["Yihua Cheng","Hui Zhang","Junchen Jiang"],"categories":null,"content":" ","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"f3f7c76563d5833bfe6b01dd5c46981b","permalink":"https://uchi-jcl.github.io/group-website/publication/onlineprofile/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/group-website/publication/onlineprofile/","section":"publication","summary":"Online profiling quality sensitivity of a video by gathering and analyzing QoE-related feedback.","tags":[],"title":"Online Profiling and Adaptation of Quality Sensitivity for Internet Video","type":"publication"},{"authors":["Chengcheng Wan","Yuhan Liu","Kuntai Du","Henry Hoffmann","Junchen Jiang","Michael Maire","Shan Lu"],"categories":null,"content":" ","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"19d10cca63bd03d4169060412171eac2","permalink":"https://uchi-jcl.github.io/group-website/publication/runtimeml/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/group-website/publication/runtimeml/","section":"publication","summary":"Tool that automatically detects and converts mismatching or incorrect ML API output at run time.","tags":[],"title":"Run-Time Prevention of Software Integration Failures of Machine Learning APIs","type":"publication"},{"authors":["Henry Milner","Yihua Cheng","Jibin Zhan","Hui Zhang","Vyas Sekar","Junchen Jiang","Ion Stoica"],"categories":null,"content":" ","date":1672617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672617600,"objectID":"1db0935296fb4a4cab15c391c11e8a2f","permalink":"https://uchi-jcl.github.io/group-website/publication/timeline/","publishdate":"2023-01-02T00:00:00Z","relpermalink":"/group-website/publication/timeline/","section":"publication","summary":"Timeline abstraction for serving video session workloads to reduce development complexity and improve cost-performance tradeoffs.","tags":[],"title":"Raising the Level of Abstraction for Time-State Analytics With the Timeline Framework","type":"publication"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://uchi-jcl.github.io/group-website/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/group-website/contact/","section":"","summary":"","tags":null,"title":"Contact Us","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://uchi-jcl.github.io/group-website/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/group-website/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":["Kuntai Du","Qizheng Zhang","Anton Arapin","Haodong Wang","Zhengxu Xia","Junchen Jiang"],"categories":null,"content":" ","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"9bac323108473649562988b0aebf37ec","permalink":"https://uchi-jcl.github.io/group-website/publication/accmpeg/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/group-website/publication/accmpeg/","section":"publication","summary":"New server-side DNN to quickly create a cheap model to infer the accuracy gradient on any new frame in near realtime.","tags":[],"title":"AccMPEG: Optimizing Video Encoding for Video Analytics","type":"publication"},{"authors":["Zhengxu Xia","Yajie Zhou","Francis Y. Yan","Junchen Jiang"],"categories":null,"content":" ","date":1659312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659312000,"objectID":"7f678be31ff7456deeab25faadd10c11","permalink":"https://uchi-jcl.github.io/group-website/publication/genet/","publishdate":"2022-08-01T00:00:00Z","relpermalink":"/group-website/publication/genet/","section":"publication","summary":"A new training framework for learning better RL-based network adaptation algorithms.","tags":[],"title":"Automatic Curriculum Generation for Learning Adaptation in Networking","type":"publication"},{"authors":["Xu Zhang","Yiyang Ou","Siddhartha Sen","Junchen Jiang"],"categories":null,"content":" ","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"4148bdadf69a105f09f4e1c7f2ebe834","permalink":"https://uchi-jcl.github.io/group-website/publication/sensei/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/group-website/publication/sensei/","section":"publication","summary":"Video streaming with dynamic quality sensitivity into existing quality adaptation algorithms.","tags":[],"title":"SENSEI: Aligning Video Streaming Quality with Dynamic User Sensitivity","type":"publication"}]